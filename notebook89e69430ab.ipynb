{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8458819,"sourceType":"datasetVersion","datasetId":5007313},{"sourceId":8467863,"sourceType":"datasetVersion","datasetId":5048767},{"sourceId":8467870,"sourceType":"datasetVersion","datasetId":5048772},{"sourceId":8467878,"sourceType":"datasetVersion","datasetId":5048779}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Library","metadata":{}},{"cell_type":"code","source":"!pip install PySastrawi\n!pip install emoji\n!pip install nltk","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-21T06:30:29.255415Z","iopub.execute_input":"2024-05-21T06:30:29.255666Z","iopub.status.idle":"2024-05-21T06:31:06.875626Z","shell.execute_reply.started":"2024-05-21T06:30:29.255643Z","shell.execute_reply":"2024-05-21T06:31:06.874551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport unicodedata\nimport re\nimport emoji\nfrom nltk.tokenize import word_tokenize\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\nfrom Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:31:06.877819Z","iopub.execute_input":"2024-05-21T06:31:06.878352Z","iopub.status.idle":"2024-05-21T06:31:08.323521Z","shell.execute_reply.started":"2024-05-21T06:31:06.878301Z","shell.execute_reply":"2024-05-21T06:31:08.322731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"# Load the uploaded file into a DataFrame\ndf = pd.read_csv(\"/kaggle/input/absa-dataset/DataABSA - id_data_raw (1).csv\")\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:31:46.434475Z","iopub.execute_input":"2024-05-21T06:31:46.434970Z","iopub.status.idle":"2024-05-21T06:31:46.490756Z","shell.execute_reply.started":"2024-05-21T06:31:46.434939Z","shell.execute_reply":"2024-05-21T06:31:46.489660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Membaca data dari file CSV\ndf = pd.read_csv(\"/kaggle/input/absa-dataset/DataABSA - id_data_raw (1).csv\")\n\n# List kolom aspek dan sentimen\naspek_cols = [f'aspek{i}' for i in range(1, 7)]\nsentimen_cols = [f'sentimen{i}' for i in range(1, 7)]\n\n# Dataframe kosong untuk hasil akhir\nresult = []\n\n# Loop melalui setiap baris\nfor index, row in df.iterrows():\n    # Loop melalui setiap pasangan aspek dan sentimen\n    for aspek_col, sentimen_col in zip(aspek_cols, sentimen_cols):\n        aspek = row[aspek_col]\n        sentimen = row[sentimen_col]\n        if pd.notna(aspek) and pd.notna(sentimen):\n            result.append({\n                'No.': row['No.'],\n                'Review': row['Review'],\n                'aspek': aspek,\n                'sentimen': sentimen\n            })\n\n# Konversi list of dicts ke DataFrame\nresult_df = pd.DataFrame(result)\n\n# Menampilkan hasil akhir\nprint(result_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:31:50.549064Z","iopub.execute_input":"2024-05-21T06:31:50.549747Z","iopub.status.idle":"2024-05-21T06:31:50.836001Z","shell.execute_reply.started":"2024-05-21T06:31:50.549714Z","shell.execute_reply":"2024-05-21T06:31:50.835079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport emoji\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\nfrom Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\nfrom nltk.tokenize import word_tokenize\nimport unicodedata\n\nclass preprocess_data:\n    def __init__(self):\n        # Load the CSV file into a DataFrame\n        df_kamusalay = pd.read_csv('/kaggle/input/kamusalay/new_kamusalay.csv', header=None, encoding='latin1')\n        self.word_map = dict(zip(df_kamusalay[0], df_kamusalay[1]))\n\n        # Load the abusive words from abusive.csv into a set\n        df_abusive = pd.read_csv('/kaggle/input/abusive/abusive.csv')\n        self.abusive_words = set(df_abusive['ABUSIVE'])\n\n        factory = StemmerFactory()\n        self.stemmer = factory.create_stemmer()\n        stopword_factory = StopWordRemoverFactory()\n        self.stopwords = stopword_factory.get_stop_words()\n\n        # Aspect terms to be preserved\n        self.aspect_terms = {'aspek', 'pelayanan', 'tempat', 'harga', 'menu', 'rasa', 'makanan'}\n\n        # Remove aspect terms from stopwords\n        self.stopwords = [word for word in self.stopwords if word not in self.aspect_terms]\n\n        # Custom stemming dictionary\n        self.custom_stem_dict = {\n            'makanannya': 'makanan',\n            'makanan': 'makanan',\n            'pelayanan': 'pelayanan',\n            'pelayanannya': 'pelayanan',\n            'pelayan': 'pelayanan',\n            'layanan': 'pelayanan'\n        }\n\n    def case_folding(self, text):\n        return text.lower()\n\n    def remove_non_ascii(self, text):\n        return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    \n    def remove_punctuation(self, text):\n        return re.sub(r'[^\\w\\s]', ' ', text)\n\n    def remove_repeated_characters(self, text):\n        return re.sub(r'\\b(\\w*?)([^grnlmo])\\2+(\\w*)\\b', r'\\1\\2\\3', text)\n\n    def fix_typos(self, text):\n        words = text.split()\n        normalized_words = [self.word_map.get(word, word) for word in words]\n        return ' '.join(normalized_words)\n    \n    def remove_abusive_words(self, text):\n        words = text.split()\n        clean_words = [word for word in words if word.lower() not in self.abusive_words]\n        return ' '.join(clean_words)\n\n    def remove_whitespace(self, text):\n        return re.sub(r'\\s+', ' ', text).strip()\n\n    def emojize(self, text):\n        return emoji.demojize(text)\n\n    def tokenize(self, text):\n        return word_tokenize(text)\n\n    def remove_stopwords(self, tokens):\n        return [word for word in tokens if word not in self.stopwords]\n\n    def stemming(self, text):\n        words = text.split()\n        stemmed_words = [self.custom_stem_dict.get(word, self.stemmer.stem(word)) for word in words]\n        return ' '.join(stemmed_words)\n    \n    def remove_numbers(self, text):\n        return re.sub(r'\\d+', '', text)\n    \n    def preprocess_text(self, text):\n        text = self.remove_non_ascii(text)\n        text = self.case_folding(text)\n        text = self.remove_punctuation(text)\n        text = self.remove_repeated_characters(text)\n        text = self.fix_typos(text)\n        text = self.remove_abusive_words(text)\n        text = self.remove_whitespace(text)\n        text = self.emojize(text)\n        text = self.remove_numbers(text)\n        tokens = self.tokenize(text)\n        tokens = self.remove_stopwords(tokens)\n        cleaned_text = ' '.join(tokens)\n        text = self.stemming(cleaned_text)\n        return text","metadata":{"execution":{"iopub.status.busy":"2024-05-21T07:02:56.595905Z","iopub.execute_input":"2024-05-21T07:02:56.596276Z","iopub.status.idle":"2024-05-21T07:02:56.615367Z","shell.execute_reply.started":"2024-05-21T07:02:56.596244Z","shell.execute_reply":"2024-05-21T07:02:56.614327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaned_data = result_df.copy()\ncleaned_data = cleaned_data.rename(columns={'Review (lower)': 'Review'})\n\ncleaned_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-21T07:02:58.397748Z","iopub.execute_input":"2024-05-21T07:02:58.398112Z","iopub.status.idle":"2024-05-21T07:02:58.410699Z","shell.execute_reply.started":"2024-05-21T07:02:58.398083Z","shell.execute_reply":"2024-05-21T07:02:58.409590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Oversampling pada sentimen\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef oversampling_and_plot(df, sentiment_col):\n    # Count occurrences of each sentiment class\n    sentiment_counts = df[sentiment_col].value_counts()\n    count_class_0_sent = sentiment_counts.iloc[0]\n\n    # Filter data for each sentiment class\n    df_class_0_sent = df[df[sentiment_col] == sentiment_counts.index[0]]\n    df_class_1_sent = df[df[sentiment_col] == sentiment_counts.index[1]]\n    df_class_2_sent = df[df[sentiment_col] == sentiment_counts.index[2]]\n\n    # Oversample the minority classes to match the count of the majority class\n    df_class_1_over_sent = df_class_1_sent.sample(count_class_0_sent, replace=True)\n    df_class_2_over_sent = df_class_2_sent.sample(count_class_0_sent, replace=True)\n\n    # Concatenate the oversampled dataframes\n    df_test_over_sent = pd.concat([df_class_0_sent, df_class_1_over_sent, df_class_2_over_sent], axis=0)\n\n    # Plotting\n    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n    # Plotting the original distribution\n    sns.countplot(ax=axes[0], data=df, x=sentiment_col)\n    axes[0].set_title('Original Sentiment Distribution')\n    axes[0].set_xlabel('Sentiment')\n    axes[0].set_ylabel('Count')\n\n    # Plotting the oversampled distribution\n    sns.countplot(ax=axes[1], data=df_test_over_sent, x=sentiment_col)\n    axes[1].set_title('Oversampled Sentiment Distribution')\n    axes[1].set_xlabel('Sentiment')\n    axes[1].set_ylabel('Count')\n\n    plt.tight_layout()\n    plt.show()\n\n    return df_test_over_sent\n\n# Example usage\ndf_cleaned = pd.DataFrame(cleaned_data)\nbalanced_df = oversampling_and_plot(df_cleaned, 'sentimen')","metadata":{"execution":{"iopub.status.busy":"2024-05-21T07:03:02.124829Z","iopub.execute_input":"2024-05-21T07:03:02.125811Z","iopub.status.idle":"2024-05-21T07:03:02.706386Z","shell.execute_reply.started":"2024-05-21T07:03:02.125768Z","shell.execute_reply":"2024-05-21T07:03:02.705267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install nlpaug\n!pip install nltk","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:53:55.454081Z","iopub.execute_input":"2024-05-21T06:53:55.454747Z","iopub.status.idle":"2024-05-21T06:54:20.224127Z","shell.execute_reply.started":"2024-05-21T06:53:55.454716Z","shell.execute_reply":"2024-05-21T06:54:20.223021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming dn is an instance of the data_normalization class\npd_instance = preprocess_data()\n\n# Apply the normalize_text method to the review column and replace the values\nbalanced_df['Review'] = balanced_df['Review'].apply(lambda x: pd_instance.preprocess_text(x))\n\nprint(balanced_df)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T07:03:09.091820Z","iopub.execute_input":"2024-05-21T07:03:09.092504Z","iopub.status.idle":"2024-05-21T07:03:20.117720Z","shell.execute_reply.started":"2024-05-21T07:03:09.092474Z","shell.execute_reply":"2024-05-21T07:03:20.116698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting Data","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\n\nclass Config:\n    def __init__(self):\n        self.batch_size = 16\n        self.val_size = 0.1\n\nconfig = Config()\n\n# Splitting the dataset\ntrain_data, val_data = train_test_split(balanced_df, test_size=config.val_size, random_state=42)\n\n# Creating DataLoader instances\ntrain_loader = DataLoader(train_data.to_dict('records'), batch_size=config.batch_size, shuffle=True)\nval_loader = DataLoader(val_data.to_dict('records'), batch_size=config.batch_size, shuffle=False)\n\n# Print to verify the splits\nprint(f\"Training set size: {len(train_data)}\")\nprint(f\"Validation set size: {len(val_data)}\")\n\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-21T07:04:04.675821Z","iopub.execute_input":"2024-05-21T07:04:04.676590Z","iopub.status.idle":"2024-05-21T07:04:04.742236Z","shell.execute_reply.started":"2024-05-21T07:04:04.676554Z","shell.execute_reply":"2024-05-21T07:04:04.741378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# Assuming cleaned_data is your dataframe\n# Convert categorical aspect labels to numerical labels\naspect_label_encoder = LabelEncoder()\nbalanced_df['aspek'] = aspect_label_encoder.fit_transform(balanced_df['aspek'])\n\n# Check the class mapping for aspects\nprint(\"Aspect Classes:\", aspect_label_encoder.classes_)\n\n# If you still need to convert 'sentimen' column to numerical labels as well\nsentiment_label_encoder = LabelEncoder()\nbalanced_df['sentimen'] = sentiment_label_encoder.fit_transform(balanced_df['sentimen'])\n\n# Check the class mapping for sentiments\nprint(\"Sentiment Classes:\", sentiment_label_encoder.classes_)\n\nbalanced_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T07:05:01.462655Z","iopub.execute_input":"2024-05-21T07:05:01.463502Z","iopub.status.idle":"2024-05-21T07:05:01.482876Z","shell.execute_reply.started":"2024-05-21T07:05:01.463468Z","shell.execute_reply":"2024-05-21T07:05:01.481986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ML Model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom tqdm import tqdm\n\n# Tokenizer\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-large-p1')\n\n# Prepare the dataset class\nclass ReviewDataset(Dataset):\n    def __init__(self, data, tokenizer, max_len):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        review = str(self.data.iloc[idx]['Review'])\n        aspect = self.data.iloc[idx]['aspek']\n\n        encoding = self.tokenizer.encode_plus(\n            review,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'aspect': torch.tensor(aspect, dtype=torch.long)\n        }\n\n# Load and preprocess the data\n# Assuming cleaned_data is already defined as a DataFrame\n\n# Encode aspects\naspect_label_encoder = LabelEncoder()\nbalanced_df['aspek'] = aspect_label_encoder.fit_transform(balanced_df['aspek'])\n\n# Prepare Data\ntrain_data, val_data = train_test_split(balanced_df, test_size=0.1, random_state=42)\ntrain_dataset = ReviewDataset(train_data, tokenizer, max_len=128)\nval_dataset = ReviewDataset(val_data, tokenizer, max_len=128)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Initialize the model\nconfig = BertConfig.from_pretrained('indobenchmark/indobert-large-p1', num_labels=len(aspect_label_encoder.classes_))\nmodel = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-large-p1', config=config)\n\n# Training\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n\nfor epoch in range(2):  \n    model.train()\n    total_loss = 0\n    predictions = []\n    true_labels = []\n\n    for batch in tqdm(train_loader):\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        aspect = batch['aspect'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=aspect)\n        loss = outputs.loss\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n        # Collect predictions and true labels for metrics\n        _, preds = torch.max(outputs.logits, dim=1)\n        predictions.extend(preds.cpu().numpy())\n        true_labels.extend(aspect.cpu().numpy())\n\n    # Calculate metrics for the current epoch\n    predicted_aspects = aspect_label_encoder.inverse_transform(predictions)\n    true_aspects = aspect_label_encoder.inverse_transform(true_labels)\n\n    accuracy = accuracy_score(true_aspects, predicted_aspects)\n    precision = precision_score(true_aspects, predicted_aspects, average='weighted')\n    recall = recall_score(true_aspects, predicted_aspects, average='weighted')\n    f1 = f1_score(true_aspects, predicted_aspects, average='weighted')\n\n    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n\n# Prediction on validation set\nmodel.eval()\npredictions = []\ntrue_labels = []\nwith torch.no_grad():\n    for batch in tqdm(val_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        true_aspects = batch['aspect'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        _, preds = torch.max(outputs.logits, dim=1)\n        predictions.extend(preds.cpu().numpy())\n        true_labels.extend(true_aspects.cpu().numpy())\n\n# Decode predictions to original aspect labels\npredicted_aspects = aspect_label_encoder.inverse_transform(predictions)\ntrue_aspects = aspect_label_encoder.inverse_transform(true_labels)\n\n# Calculate metrics\naccuracy = accuracy_score(true_aspects, predicted_aspects)\nprecision = precision_score(true_aspects, predicted_aspects, average='weighted')\nrecall = recall_score(true_aspects, predicted_aspects, average='weighted')\nf1 = f1_score(true_aspects, predicted_aspects, average='weighted')\n\nprint(f\"Validation Accuracy: {accuracy:.4f}\")\nprint(f\"Validation Precision: {precision:.4f}\")\nprint(f\"Validation Recall: {recall:.4f}\")\nprint(f\"Validation F1 Score: {f1:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T07:06:24.960764Z","iopub.execute_input":"2024-05-21T07:06:24.961146Z","iopub.status.idle":"2024-05-21T07:20:23.310482Z","shell.execute_reply.started":"2024-05-21T07:06:24.961116Z","shell.execute_reply":"2024-05-21T07:20:23.309316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom tqdm import tqdm\n\n# Configuration\nclass config:\n    def __init__(self):\n        self.batch_size = 16\n        self.val_size = 0.1\n        self.learning_rate = 5e-5\n        self.epochs = 2\n        self.max_len = 128\n\nconfig = Config()\n\n# Encode labels\naspect_label_encoder = LabelEncoder()\nsentiment_label_encoder = LabelEncoder()\ncleaned_data['aspek'] = aspect_label_encoder.fit_transform(cleaned_data['aspek'])\ncleaned_data['sentimen'] = sentiment_label_encoder.fit_transform(cleaned_data['sentimen'])\n\n# Tokenizer\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n\n# Dataset class\nclass ReviewDataset(Dataset):\n    def __init__(self, data, tokenizer, max_len):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        review = str(self.data.iloc[idx]['Review'])\n        aspect = self.data.iloc[idx]['aspek']\n\n        encoding = self.tokenizer.encode_plus(\n            review,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'aspect': torch.tensor(aspect, dtype=torch.long)\n        }\n\n# Split data and create DataLoaders\ntrain_data, val_data = train_test_split(cleaned_data, test_size=config.val_size, random_state=42)\ntrain_dataset = ReviewDataset(train_data, tokenizer, config.max_len)\nval_dataset = ReviewDataset(val_data, tokenizer, config.max_len)\ntrain_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n\n# Initialize model\nmodel_config = BertConfig.from_pretrained('indobenchmark/indobert-base-p1', num_labels=len(aspect_label_encoder.classes_))\nmodel = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', config=model_config)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=config.learning_rate)\n\n# Training function\ndef train_epoch(model, data_loader, optimizer, device):\n    model.train()\n    total_loss = 0\n    predictions, true_labels = [], []\n\n    for batch in tqdm(data_loader):\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        aspect = batch['aspect'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=aspect)\n        loss = outputs.loss\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n        _, preds = torch.max(outputs.logits, dim=1)\n        predictions.extend(preds.cpu().numpy())\n        true_labels.extend(aspect.cpu().numpy())\n\n    return total_loss, predictions, true_labels\n\n# Evaluation function\ndef evaluate_model(model, data_loader, device):\n    model.eval()\n    predictions, true_labels = [], []\n\n    with torch.no_grad():\n        for batch in tqdm(data_loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            aspect = batch['aspect'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n            _, preds = torch.max(outputs.logits, dim=1)\n            predictions.extend(preds.cpu().numpy())\n            true_labels.extend(aspect.cpu().numpy())\n\n    return predictions, true_labels\n\n# Training loop\nfor epoch in range(config.epochs):\n    total_loss, train_predictions, train_labels = train_epoch(model, train_loader, optimizer, device)\n    train_accuracy = accuracy_score(train_labels, train_predictions)\n    train_precision = precision_score(train_labels, train_predictions, average='weighted')\n    train_recall = recall_score(train_labels, train_predictions, average='weighted')\n    train_f1 = f1_score(train_labels, train_predictions, average='weighted')\n\n    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}, \"\n          f\"Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, \"\n          f\"Recall: {train_recall:.4f}, F1 Score: {train_f1:.4f}\")\n\n# Validation\nval_predictions, val_labels = evaluate_model(model, val_loader, device)\nval_accuracy = accuracy_score(val_labels, val_predictions)\nval_precision = precision_score(val_labels, val_predictions, average='weighted')\nval_recall = recall_score(val_labels, val_predictions, average='weighted')\nval_f1 = f1_score(val_labels, val_predictions, average='weighted')\n\nprint(f\"Validation Accuracy: {val_accuracy:.4f}\")\nprint(f\"Validation Precision: {val_precision:.4f}\")\nprint(f\"Validation Recall: {val_recall:.4f}\")\nprint(f\"Validation F1 Score: {val_f1:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T15:59:41.254948Z","iopub.execute_input":"2024-05-20T15:59:41.255311Z","iopub.status.idle":"2024-05-20T15:59:41.280843Z","shell.execute_reply.started":"2024-05-20T15:59:41.255281Z","shell.execute_reply":"2024-05-20T15:59:41.279486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Encode label aspek\nlabel_encoder = LabelEncoder()\ncleaned_data['sentimen'] = label_encoder.fit_transform(cleaned_data['aspek'])\n\n# Pisahkan data ke dalam set pelatihan dan validasi\ntrain_df, val_df = train_test_split(cleaned_data, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T15:59:41.282036Z","iopub.status.idle":"2024-05-20T15:59:41.282576Z","shell.execute_reply.started":"2024-05-20T15:59:41.282301Z","shell.execute_reply":"2024-05-20T15:59:41.282323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer\n\nclass ReviewDataset(Dataset):\n    def __init__(self, data, tokenizer, max_len):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        review = self.data.iloc[index]['Review']\n        labels = self.data.iloc[index]['sentimen']\n        encoding = self.tokenizer.encode_plus(\n            review,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        return {\n            'review_text': review,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(labels, dtype=torch.long)\n        }\n\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\nmax_len = 128\nbatch_size = 32\n\ntrain_dataset = ReviewDataset(train_df, tokenizer, max_len)\nval_dataset = ReviewDataset(val_df, tokenizer, max_len)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T15:59:41.284892Z","iopub.status.idle":"2024-05-20T15:59:41.285452Z","shell.execute_reply.started":"2024-05-20T15:59:41.285168Z","shell.execute_reply":"2024-05-20T15:59:41.285190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nfrom transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n\n# Load model\nmodel = BertForSequenceClassification.from_pretrained(\n    'indobenchmark/indobert-base-p1',\n    num_labels=len(label_encoder.classes_)\n)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel = model.to(device)\n\n# Define optimizer, loss function, and scheduler\noptimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\ntotal_steps = len(train_loader) * 3\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\nloss_fn = nn.CrossEntropyLoss().to(device)\n\n# Training function\ndef train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n    model.train()\n    losses = []\n    correct_predictions = 0\n    for d in data_loader:\n        input_ids = d['input_ids'].to(device)\n        attention_mask = d['attention_mask'].to(device)\n        labels = d['labels'].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        logits = outputs.logits\n\n        _, preds = torch.max(logits, dim=1)\n        correct_predictions += torch.sum(preds == labels)\n        losses.append(loss.item())\n\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n    \n    return correct_predictions.double() / n_examples, np.mean(losses)\n\n# Evaluation function\ndef eval_model(model, data_loader, loss_fn, device, n_examples):\n    model.eval()\n    losses = []\n    correct_predictions = 0\n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d['input_ids'].to(device)\n            attention_mask = d['attention_mask'].to(device)\n            labels = d['labels'].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            logits = outputs.logits\n\n            _, preds = torch.max(logits, dim=1)\n            correct_predictions += torch.sum(preds == labels)\n            losses.append(loss.item())\n\n    return correct_predictions.double() / n_examples, np.mean(losses)\n\n# Training loop\nepochs = 2\n\nfor epoch in range(epochs):\n    print(f'Epoch {epoch + 1}/{epochs}')\n    print('-' * 10)\n    \n    train_acc, train_loss = train_epoch(\n        model,\n        train_loader,\n        loss_fn,\n        optimizer,\n        device,\n        scheduler,\n        len(train_df)\n    )\n    \n    print(f'Train loss {train_loss} accuracy {train_acc}')\n    \n    val_acc, val_loss = eval_model(\n        model,\n        val_loader,\n        loss_fn,\n        device,\n        len(val_df)\n    )\n    \n    print(f'Validation loss {val_loss} accuracy {val_acc}')\n    print()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T15:59:41.287260Z","iopub.status.idle":"2024-05-20T15:59:41.287713Z","shell.execute_reply.started":"2024-05-20T15:59:41.287497Z","shell.execute_reply":"2024-05-20T15:59:41.287514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install setfit\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T15:59:41.289437Z","iopub.status.idle":"2024-05-20T15:59:41.289945Z","shell.execute_reply.started":"2024-05-20T15:59:41.289681Z","shell.execute_reply":"2024-05-20T15:59:41.289703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from setfit import SetFitModel\nfrom sklearn.preprocessing import LabelEncoder\n\n# Install SetFit library\n# pip install setfit\n\n# Load the SetFit model from the Hugging Face Hub\nmodel = SetFitModel.from_pretrained(\"firqaaa/indo-setfit-bert-base-p1\")\n\n# Example input: list of review texts\nreviews = [\n    \"Saya sangat suka dengan pelayanan di restoran ini.\",\n    \"Makanannya enak tetapi harganya terlalu mahal.\",\n    \"Lokasi restoran ini sangat strategis dan nyaman.\"\n]\n\n# Run inference\npreds = model(reviews)\n\n# Assuming you have the label encoder used during training\naspect_label_encoder = LabelEncoder()\naspect_labels = [\"lokasi\", \"harga\", \"pelayanan\", \"makanan\"]  # Example aspect labels\naspect_label_encoder.fit(aspect_labels)\n\n# Decode predictions\ndecoded_preds = aspect_label_encoder.inverse_transform(preds)\n\n# Print decoded predictions\nfor review, aspect in zip(reviews, decoded_preds):\n    print(f\"Review: {review}\")\n    print(f\"Predicted Aspect: {aspect}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T15:59:41.292039Z","iopub.status.idle":"2024-05-20T15:59:41.292407Z","shell.execute_reply.started":"2024-05-20T15:59:41.292224Z","shell.execute_reply":"2024-05-20T15:59:41.292239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Elfira Nyoba","metadata":{}},{"cell_type":"markdown","source":"## DeBERTa","metadata":{}},{"cell_type":"code","source":"!pip install transformers[sentencepiece]","metadata":{"execution":{"iopub.status.busy":"2024-05-20T18:16:47.033645Z","iopub.execute_input":"2024-05-20T18:16:47.034464Z","iopub.status.idle":"2024-05-20T18:17:00.915113Z","shell.execute_reply.started":"2024-05-20T18:16:47.034430Z","shell.execute_reply":"2024-05-20T18:17:00.913782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    def __init__(self, vocab_size=50265, hidden_size=768, num_hidden_layers=12, num_attention_heads=12,\n                 intermediate_size=3072, hidden_act='gelu', hidden_dropout_prob=0.1,\n                 attention_probs_dropout_prob=0.1, max_position_embeddings=512, type_vocab_size=0,\n                 initializer_range=0.02, layer_norm_eps=1e-07, relative_attention=False,\n                 max_relative_positions=-1, pad_token_id=0, position_biased_input=True,\n                 pos_att_type=None, pooler_dropout=0, pooler_hidden_act='gelu', **kwargs):\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.intermediate_size = intermediate_size\n        self.hidden_act = hidden_act\n        self.hidden_dropout_prob = hidden_dropout_prob\n        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n        self.max_position_embeddings = max_position_embeddings\n        self.type_vocab_size = type_vocab_size\n        self.initializer_range = initializer_range\n        self.layer_norm_eps = layer_norm_eps\n        self.relative_attention = relative_attention\n        self.max_relative_positions = max_relative_positions\n        self.pad_token_id = pad_token_id\n        self.position_biased_input = position_biased_input\n        self.pos_att_type = pos_att_type\n        self.pooler_dropout = pooler_dropout\n        self.pooler_hidden_act = pooler_hidden_act\n        self.extra_args = kwargs\n\n    def __repr__(self):\n        return (f\"ModelConfig(vocab_size={self.vocab_size}, hidden_size={self.hidden_size}, \"\n                f\"num_hidden_layers={self.num_hidden_layers}, num_attention_heads={self.num_attention_heads}, \"\n                f\"intermediate_size={self.intermediate_size}, hidden_act='{self.hidden_act}', \"\n                f\"hidden_dropout_prob={self.hidden_dropout_prob}, attention_probs_dropout_prob={self.attention_probs_dropout_prob}, \"\n                f\"max_position_embeddings={self.max_position_embeddings}, type_vocab_size={self.type_vocab_size}, \"\n                f\"initializer_range={self.initializer_range}, layer_norm_eps={self.layer_norm_eps}, \"\n                f\"relative_attention={self.relative_attention}, max_relative_positions={self.max_relative_positions}, \"\n                f\"pad_token_id={self.pad_token_id}, position_biased_input={self.position_biased_input}, \"\n                f\"pos_att_type={self.pos_att_type}, pooler_dropout={self.pooler_dropout}, \"\n                f\"pooler_hidden_act='{self.pooler_hidden_act}', extra_args={self.extra_args})\")\n\n# Usage\nconfig = Config()\nprint(config)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T18:26:18.948425Z","iopub.execute_input":"2024-05-20T18:26:18.949235Z","iopub.status.idle":"2024-05-20T18:26:18.962023Z","shell.execute_reply.started":"2024-05-20T18:26:18.949183Z","shell.execute_reply":"2024-05-20T18:26:18.960722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, DebertaModel\nimport torch\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom tqdm import tqdm\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n\n# Prepare the dataset class\nclass ReviewDataset(Dataset):\n    def __init__(self, data, tokenizer, max_len):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        review = str(self.data.iloc[idx]['Review'])\n        aspect = self.data.iloc[idx]['aspek']\n\n        encoding = self.tokenizer.encode_plus(\n            review,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'aspect': torch.tensor(aspect, dtype=torch.long)\n        }\n\n# Load and preprocess the data\n# Assuming cleaned_data is already defined as a DataFrame\n\n# Encode aspects\naspect_label_encoder = LabelEncoder()\ncleaned_data['aspek'] = aspect_label_encoder.fit_transform(cleaned_data['aspek'])\n\n# Prepare Data\ntrain_data, val_data = train_test_split(cleaned_data, test_size=0.1, random_state=42)\ntrain_dataset = ReviewDataset(train_data, tokenizer, max_len=128)\nval_dataset = ReviewDataset(val_data, tokenizer, max_len=128)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Initialize the model\nconfiguration = Config()\n\n# Initializing a model (with random weights) from the microsoft/deberta-base style configuration\nmodel = DebertaModel.from_pretrained(\"microsoft/deberta-base\")\n\n# Accessing the model configuration\nconfiguration = model.config\n\n# Training\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.torch(device)\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n\nfor epoch in range(2):  \n    model.train()\n    total_loss = 0\n    predictions = []\n    true_labels = []\n\n    for batch in tqdm(train_loader):\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].torch(device)\n        attention_mask = batch['attention_mask'].torch(device)\n        aspect = batch['aspect'].torch(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=aspect)\n        loss = outputs.loss\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n        # Collect predictions and true labels for metrics\n        _, preds = torch.max(outputs.logits, dim=1)\n        predictions.extend(preds.cpu().numpy())\n        true_labels.extend(aspect.cpu().numpy())\n\n    # Calculate metrics for the current epoch\n    predicted_aspects = aspect_label_encoder.inverse_transform(predictions)\n    true_aspects = aspect_label_encoder.inverse_transform(true_labels)\n\n    accuracy = accuracy_score(true_aspects, predicted_aspects)\n    precision = precision_score(true_aspects, predicted_aspects, average='weighted')\n    recall = recall_score(true_aspects, predicted_aspects, average='weighted')\n    f1 = f1_score(true_aspects, predicted_aspects, average='weighted')\n\n    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n\n# Prediction on validation set\nmodel.eval()\npredictions = []\ntrue_labels = []\nwith torch.no_grad():\n    for batch in tqdm(val_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        true_aspects = batch['aspect'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        _, preds = torch.max(outputs.logits, dim=1)\n        predictions.extend(preds.cpu().numpy())\n        true_labels.extend(true_aspects.cpu().numpy())\n\n# Decode predictions to original aspect labels\npredicted_aspects = aspect_label_encoder.inverse_transform(predictions)\ntrue_aspects = aspect_label_encoder.inverse_transform(true_labels)\n\n# Calculate metrics\naccuracy = accuracy_score(true_aspects, predicted_aspects)\nprecision = precision_score(true_aspects, predicted_aspects, average='weighted')\nrecall = recall_score(true_aspects, predicted_aspects, average='weighted')\nf1 = f1_score(true_aspects, predicted_aspects, average='weighted')\n\nprint(f\"Validation Accuracy: {accuracy:.4f}\")\nprint(f\"Validation Precision: {precision:.4f}\")\nprint(f\"Validation Recall: {recall:.4f}\")\nprint(f\"Validation F1 Score: {f1:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-20T18:48:37.862221Z","iopub.execute_input":"2024-05-20T18:48:37.862619Z","iopub.status.idle":"2024-05-20T18:48:38.561407Z","shell.execute_reply.started":"2024-05-20T18:48:37.862588Z","shell.execute_reply":"2024-05-20T18:48:38.559999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n\nfrom transformers import AutoTokenizer, DebertaForSequenceClassification\nimport torch\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom tqdm import tqdm\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n\n# Prepare the dataset class\nclass ReviewDataset(Dataset):\n    def __init__(self, data, tokenizer, max_len):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        review = str(self.data.iloc[idx]['Review'])\n        aspect = self.data.iloc[idx]['aspek']\n\n        encoding = self.tokenizer.encode_plus(\n            review,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'aspect': torch.tensor(aspect, dtype=torch.long)\n        }\n\n# Load and preprocess the data\n# Assuming cleaned_data is already defined as a DataFrame\n\n# Encode aspects\naspect_label_encoder = LabelEncoder()\ncleaned_data['aspek'] = aspect_label_encoder.fit_transform(cleaned_data['aspek'])\n\n# Check for invalid labels\nnum_labels = len(aspect_label_encoder.classes_)\nassert cleaned_data['aspek'].min() >= 0 and cleaned_data['aspek'].max() < num_labels, \"Invalid labels detected\"\n\n# Prepare Data\ntrain_data, val_data = train_test_split(cleaned_data, test_size=0.1, random_state=42)\ntrain_dataset = ReviewDataset(train_data, tokenizer, max_len=128)\nval_dataset = ReviewDataset(val_data, tokenizer, max_len=128)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Initialize the model\nmodel = DebertaForSequenceClassification.from_pretrained(\"microsoft/deberta-base\", num_labels=num_labels)\n\n# Training setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training loop\nepochs = 2  # Adjust the number of epochs as needed\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    predictions = []\n    true_labels = []\n\n    for batch in tqdm(train_loader):\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        aspects = batch['aspect'].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=aspects)\n        loss = outputs.loss\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n        # Collect predictions and true labels for metrics\n        _, preds = torch.max(outputs.logits, dim=1)\n        predictions.extend(preds.cpu().numpy())\n        true_labels.extend(aspects.cpu().numpy())\n\n    # Calculate metrics for the current epoch\n    predicted_aspects = aspect_label_encoder.inverse_transform(predictions)\n    true_aspects = aspect_label_encoder.inverse_transform(true_labels)\n\n    accuracy = accuracy_score(true_aspects, predicted_aspects)\n    precision = precision_score(true_aspects, predicted_aspects, average='weighted')\n    recall = recall_score(true_aspects, predicted_aspects, average='weighted')\n    f1 = f1_score(true_aspects, predicted_aspects, average='weighted')\n\n    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n\n# Prediction on validation set\nmodel.eval()\npredictions = []\ntrue_labels = []\nwith torch.no_grad():\n    for batch in tqdm(val_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        aspects = batch['aspect'].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        _, preds = torch.max(outputs.logits, dim=1)\n        predictions.extend(preds.cpu().numpy())\n        true_labels.extend(aspects.cpu().numpy())\n\n# Decode predictions to original aspect labels\npredicted_aspects = aspect_label_encoder.inverse_transform(predictions)\ntrue_aspects = aspect_label_encoder.inverse_transform(true_labels)\n\n# Calculate metrics\naccuracy = accuracy_score(true_aspects, predicted_aspects)\nprecision = precision_score(true_aspects, predicted_aspects, average='weighted')\nrecall = recall_score(true_aspects, predicted_aspects, average='weighted')\nf1 = f1_score(true_aspects, predicted_aspects, average='weighted')\n\nprint(f\"Validation Accuracy: {accuracy:.4f}\")\nprint(f\"Validation Precision: {precision:.4f}\")\nprint(f\"Validation Recall: {recall:.4f}\")\nprint(f\"Validation F1 Score: {f1:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T18:46:50.541497Z","iopub.execute_input":"2024-05-20T18:46:50.542487Z","iopub.status.idle":"2024-05-20T18:46:52.025084Z","shell.execute_reply.started":"2024-05-20T18:46:50.542450Z","shell.execute_reply":"2024-05-20T18:46:52.023610Z"},"trusted":true},"execution_count":null,"outputs":[]}]}