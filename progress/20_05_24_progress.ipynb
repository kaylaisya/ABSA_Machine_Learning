{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8458819,"sourceType":"datasetVersion","datasetId":5007313}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install PySastrawi\n!pip install emoji\n!pip install nltk","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-19T20:25:52.645924Z","iopub.execute_input":"2024-05-19T20:25:52.646289Z","iopub.status.idle":"2024-05-19T20:26:30.389015Z","shell.execute_reply.started":"2024-05-19T20:25:52.646257Z","shell.execute_reply":"2024-05-19T20:26:30.387909Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting PySastrawi\n  Downloading PySastrawi-1.2.0-py2.py3-none-any.whl.metadata (892 bytes)\nDownloading PySastrawi-1.2.0-py2.py3-none-any.whl (210 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.6/210.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: PySastrawi\nSuccessfully installed PySastrawi-1.2.0\nRequirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (2.11.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport unicodedata\nimport re\nimport emoji\nfrom nltk.tokenize import word_tokenize\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\nfrom Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory","metadata":{"execution":{"iopub.status.busy":"2024-05-19T20:26:30.390784Z","iopub.execute_input":"2024-05-19T20:26:30.391054Z","iopub.status.idle":"2024-05-19T20:26:31.835772Z","shell.execute_reply.started":"2024-05-19T20:26:30.391023Z","shell.execute_reply":"2024-05-19T20:26:31.834998Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Load the uploaded file into a DataFrame\ndf = pd.read_csv(\"/kaggle/input/absa-data/DataABSA - id_data_raw (1).csv\")\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T20:26:31.836845Z","iopub.execute_input":"2024-05-19T20:26:31.837245Z","iopub.status.idle":"2024-05-19T20:26:31.890362Z","shell.execute_reply.started":"2024-05-19T20:26:31.837219Z","shell.execute_reply":"2024-05-19T20:26:31.889510Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   No.                                             Review aspek1 sentimen1  \\\n0    1  makanannya enak enak, pelayanannya juga mantap...    NaN       NaN   \n1    2  Rasanya ga pernah bisa bohong, setiap ke McD s...   rasa   positif   \n2    3  Rasanya selalu enak gak pernah berubah, higine...   rasa   positif   \n3    4  Pelayanannya cukup ramah dan praktis. Makanann...    NaN       NaN   \n4    5  Tempatnya nyaman bersih ,makanannya recommende...    NaN       NaN   \n\n   aspek2 sentimen2 aspek3 sentimen3     aspek4 sentimen4 aspek5 sentimen5  \\\n0     NaN       NaN    NaN       NaN  pelayanan   positif    NaN       NaN   \n1     NaN       NaN    NaN       NaN        NaN       NaN   menu   positif   \n2     NaN       NaN    NaN       NaN        NaN       NaN    NaN       NaN   \n3     NaN       NaN    NaN       NaN  pelayanan   positif    NaN       NaN   \n4  tempat   positif    NaN       NaN        NaN       NaN    NaN       NaN   \n\n    aspek6 sentimen6  \n0  makanan   positif  \n1      NaN       NaN  \n2      NaN       NaN  \n3  makanan   positif  \n4  makanan   positif  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>No.</th>\n      <th>Review</th>\n      <th>aspek1</th>\n      <th>sentimen1</th>\n      <th>aspek2</th>\n      <th>sentimen2</th>\n      <th>aspek3</th>\n      <th>sentimen3</th>\n      <th>aspek4</th>\n      <th>sentimen4</th>\n      <th>aspek5</th>\n      <th>sentimen5</th>\n      <th>aspek6</th>\n      <th>sentimen6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>makanannya enak enak, pelayanannya juga mantap...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>pelayanan</td>\n      <td>positif</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>makanan</td>\n      <td>positif</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Rasanya ga pernah bisa bohong, setiap ke McD s...</td>\n      <td>rasa</td>\n      <td>positif</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>menu</td>\n      <td>positif</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Rasanya selalu enak gak pernah berubah, higine...</td>\n      <td>rasa</td>\n      <td>positif</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Pelayanannya cukup ramah dan praktis. Makanann...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>pelayanan</td>\n      <td>positif</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>makanan</td>\n      <td>positif</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Tempatnya nyaman bersih ,makanannya recommende...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>tempat</td>\n      <td>positif</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>makanan</td>\n      <td>positif</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\n# Membaca data dari file CSV\ndf = pd.read_csv(\"/kaggle/input/absa-data/DataABSA - id_data_raw (1).csv\")\n\n# List kolom aspek dan sentimen\naspek_cols = [f'aspek{i}' for i in range(1, 7)]\nsentimen_cols = [f'sentimen{i}' for i in range(1, 7)]\n\n# Dataframe kosong untuk hasil akhir\nresult = []\n\n# Loop melalui setiap baris\nfor index, row in df.iterrows():\n    # Loop melalui setiap pasangan aspek dan sentimen\n    for aspek_col, sentimen_col in zip(aspek_cols, sentimen_cols):\n        aspek = row[aspek_col]\n        sentimen = row[sentimen_col]\n        if pd.notna(aspek) and pd.notna(sentimen):\n            result.append({\n                'No.': row['No.'],\n                'Review': row['Review'],\n                'aspek': aspek,\n                'sentimen': sentimen\n            })\n\n# Konversi list of dicts ke DataFrame\nresult_df = pd.DataFrame(result)\n\n# Menampilkan hasil akhir\nprint(result_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-19T20:26:31.892336Z","iopub.execute_input":"2024-05-19T20:26:31.892622Z","iopub.status.idle":"2024-05-19T20:26:32.206621Z","shell.execute_reply.started":"2024-05-19T20:26:31.892596Z","shell.execute_reply":"2024-05-19T20:26:32.205706Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"       No.                                             Review      aspek  \\\n0        1  makanannya enak enak, pelayanannya juga mantap...  pelayanan   \n1        1  makanannya enak enak, pelayanannya juga mantap...    makanan   \n2        2  Rasanya ga pernah bisa bohong, setiap ke McD s...       rasa   \n3        2  Rasanya ga pernah bisa bohong, setiap ke McD s...       menu   \n4        3  Rasanya selalu enak gak pernah berubah, higine...       rasa   \n...    ...                                                ...        ...   \n3702  1990  pelayanan buruk banget . Makanan enakk cuma di...    makanan   \n3703  1991                   Tempat luar biasa untuk keluarga     tempat   \n3704  1992   Suasana nya nyaman,bersih..makanan nya juga enak    makanan   \n3705  1993  Tempatnya enak buat makan rame2, keluarga, ram...     tempat   \n3706  1993  Tempatnya enak buat makan rame2, keluarga, ram...    makanan   \n\n     sentimen  \n0     positif  \n1     positif  \n2     positif  \n3     positif  \n4     positif  \n...       ...  \n3702  positif  \n3703  positif  \n3704  positif  \n3705  positif  \n3706  positif  \n\n[3707 rows x 4 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport emoji\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\nfrom Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\nfrom nltk.tokenize import word_tokenize\nimport unicodedata\n\nclass preprocess_data:\n    def __init__(self):\n        # Load the CSV file into a DataFrame\n        df_kamusalay = pd.read_csv('/kaggle/input/absa-data/new_kamusalay.csv', header=None, encoding='latin1')\n        self.word_map = dict(zip(df_kamusalay[0], df_kamusalay[1]))\n\n        # Load the abusive words from abusive.csv into a set\n        df_abusive = pd.read_csv('/kaggle/input/absa-data/abusive.csv')\n        self.abusive_words = set(df_abusive['ABUSIVE'])\n\n        factory = StemmerFactory()\n        self.stemmer = factory.create_stemmer()\n        stopword_factory = StopWordRemoverFactory()\n        self.stopwords = stopword_factory.get_stop_words()\n\n        # Aspect terms to be preserved\n        self.aspect_terms = {'aspek', 'pelayanan', 'tempat', 'harga', 'menu', 'rasa', 'makanan'}\n\n        # Remove aspect terms from stopwords\n        self.stopwords = [word for word in self.stopwords if word not in self.aspect_terms]\n\n        # Custom stemming dictionary\n        self.custom_stem_dict = {\n            'makanannya': 'makanan',\n            'makanan': 'makanan',\n            'pelayanan': 'pelayanan',\n            'pelayanannya': 'pelayanan',\n            'pelayan': 'pelayanan',\n            'layanan': 'pelayanan'\n        }\n\n    def case_folding(self, text):\n        return text.lower()\n\n    def remove_non_ascii(self, text):\n        return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    \n    def remove_punctuation(self, text):\n        return re.sub(r'[^\\w\\s]', ' ', text)\n\n    def remove_repeated_characters(self, text):\n        return re.sub(r'\\b(\\w*?)([^grnlmo])\\2+(\\w*)\\b', r'\\1\\2\\3', text)\n\n    def fix_typos(self, text):\n        words = text.split()\n        normalized_words = [self.word_map.get(word, word) for word in words]\n        return ' '.join(normalized_words)\n    \n    def remove_abusive_words(self, text):\n        words = text.split()\n        clean_words = [word for word in words if word.lower() not in self.abusive_words]\n        return ' '.join(clean_words)\n\n    def remove_whitespace(self, text):\n        return re.sub(r'\\s+', ' ', text).strip()\n\n    def emojize(self, text):\n        return emoji.demojize(text)\n\n    def tokenize(self, text):\n        return word_tokenize(text)\n\n    def remove_stopwords(self, tokens):\n        return [word for word in tokens if word not in self.stopwords]\n\n    def stemming(self, text):\n        words = text.split()\n        stemmed_words = [self.custom_stem_dict.get(word, self.stemmer.stem(word)) for word in words]\n        return ' '.join(stemmed_words)\n    \n    def remove_numbers(self, text):\n        return re.sub(r'\\d+', '', text)\n    \n    def preprocess_text(self, text):\n        text = self.remove_non_ascii(text)\n        text = self.case_folding(text)\n        text = self.remove_punctuation(text)\n        text = self.remove_repeated_characters(text)\n        text = self.fix_typos(text)\n        text = self.remove_abusive_words(text)\n        text = self.remove_whitespace(text)\n        text = self.emojize(text)\n        text = self.remove_numbers(text)\n        tokens = self.tokenize(text)\n        tokens = self.remove_stopwords(tokens)\n        cleaned_text = ' '.join(tokens)\n        text = self.stemming(cleaned_text)\n        return text","metadata":{"execution":{"iopub.status.busy":"2024-05-19T20:26:32.208206Z","iopub.execute_input":"2024-05-19T20:26:32.208573Z","iopub.status.idle":"2024-05-19T20:26:32.232867Z","shell.execute_reply.started":"2024-05-19T20:26:32.208540Z","shell.execute_reply":"2024-05-19T20:26:32.231881Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"cleaned_data = result_df.copy()\ncleaned_data = cleaned_data.rename(columns={'Review (lower)': 'Review'})\n\ncleaned_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T20:26:32.235546Z","iopub.execute_input":"2024-05-19T20:26:32.236991Z","iopub.status.idle":"2024-05-19T20:26:32.253962Z","shell.execute_reply.started":"2024-05-19T20:26:32.236965Z","shell.execute_reply":"2024-05-19T20:26:32.253130Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   No.                                             Review      aspek sentimen\n0    1  makanannya enak enak, pelayanannya juga mantap...  pelayanan  positif\n1    1  makanannya enak enak, pelayanannya juga mantap...    makanan  positif\n2    2  Rasanya ga pernah bisa bohong, setiap ke McD s...       rasa  positif\n3    2  Rasanya ga pernah bisa bohong, setiap ke McD s...       menu  positif\n4    3  Rasanya selalu enak gak pernah berubah, higine...       rasa  positif","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>No.</th>\n      <th>Review</th>\n      <th>aspek</th>\n      <th>sentimen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>makanannya enak enak, pelayanannya juga mantap...</td>\n      <td>pelayanan</td>\n      <td>positif</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>makanannya enak enak, pelayanannya juga mantap...</td>\n      <td>makanan</td>\n      <td>positif</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Rasanya ga pernah bisa bohong, setiap ke McD s...</td>\n      <td>rasa</td>\n      <td>positif</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>Rasanya ga pernah bisa bohong, setiap ke McD s...</td>\n      <td>menu</td>\n      <td>positif</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>Rasanya selalu enak gak pernah berubah, higine...</td>\n      <td>rasa</td>\n      <td>positif</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"!pip install nlpaug\n!pip install nltk","metadata":{"execution":{"iopub.status.busy":"2024-05-19T20:48:21.497938Z","iopub.execute_input":"2024-05-19T20:48:21.498750Z","iopub.status.idle":"2024-05-19T20:48:45.622902Z","shell.execute_reply.started":"2024-05-19T20:48:21.498715Z","shell.execute_reply":"2024-05-19T20:48:45.621757Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nlpaug in /opt/conda/lib/python3.10/site-packages (1.1.11)\nRequirement already satisfied: numpy>=1.16.2 in /opt/conda/lib/python3.10/site-packages (from nlpaug) (1.26.4)\nRequirement already satisfied: pandas>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from nlpaug) (2.1.4)\nRequirement already satisfied: requests>=2.22.0 in /opt/conda/lib/python3.10/site-packages (from nlpaug) (2.31.0)\nRequirement already satisfied: gdown>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from nlpaug) (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (3.13.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (4.66.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2023.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (2024.2.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.16.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import nlpaug.augmenter.word as naw\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport nltk\n\n# Mengunduh corpus WordNet menggunakan NLTK\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\n# Mengatur lokasi penyimpanan resource NLTK secara manual (opsional)\nimport os\nnltk.data.path.append('/root/nltk_data')\n\n# Inisialisasi augmenter\naug = naw.SynonymAug(aug_src='wordnet')\n\n# Fungsi untuk melakukan augmentasi teks\ndef augment_text(text):\n    return aug.augment(text)\n\n# Menerapkan augmentasi pada keseluruhan kolom 'Review'\ncleaned_data['Augmented_Review'] = cleaned_data['Review'].apply(augment_text)\n\n# Mengubah kolom Augmented_Review ke bentuk string\ncleaned_data['Augmented_Review'] = cleaned_data['Augmented_Review'].apply(lambda x: ' '.join(x))\n\n# Menggabungkan data asli dan augmented\naugmented_data = cleaned_data.copy()\naugmented_data['Review'] = augmented_data['Augmented_Review']\naugmented_data = augmented_data.drop(columns=['Augmented_Review'])\n\nfinal_data = pd.concat([cleaned_data.drop(columns=['Augmented_Review']), augmented_data])\n\n# Melakukan encode aspek\naspect_label_encoder = LabelEncoder()\nfinal_data['aspek'] = aspect_label_encoder.fit_transform(final_data['aspek'])\n\n# Melihat data setelah augmentasi dan preprocessing\nprint(final_data.head())","metadata":{"execution":{"iopub.status.busy":"2024-05-19T20:50:08.167455Z","iopub.execute_input":"2024-05-19T20:50:08.168122Z","iopub.status.idle":"2024-05-19T20:50:08.477236Z","shell.execute_reply.started":"2024-05-19T20:50:08.168089Z","shell.execute_reply":"2024-05-19T20:50:08.475841Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:80\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzip_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet.zip/wordnet/.zip/' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/root/nltk_data'\n**********************************************************************","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m aug\u001b[38;5;241m.\u001b[39maugment(text)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Menerapkan augmentasi pada keseluruhan kolom 'Review'\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m cleaned_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAugmented_Review\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcleaned_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mReview\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugment_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Mengubah kolom Augmented_Review ke bentuk string\u001b[39;00m\n\u001b[1;32m     25\u001b[0m cleaned_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAugmented_Review\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m cleaned_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAugmented_Review\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n","File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","Cell \u001b[0;32mIn[19], line 19\u001b[0m, in \u001b[0;36maugment_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maugment_text\u001b[39m(text):\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maug\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nlpaug/base_augmenter.py:119\u001b[0m, in \u001b[0;36mAugmenter.augment\u001b[0;34m(self, data, n, num_thread)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Single input with/without multiple input\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Single Thread\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_thread \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m         augmented_results \u001b[38;5;241m=\u001b[39m [action_fx(clean_data) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# Multi Thread\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m         augmented_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parallel_augment(action_fx, clean_data, n\u001b[38;5;241m=\u001b[39mn, num_thread\u001b[38;5;241m=\u001b[39mnum_thread)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nlpaug/base_augmenter.py:119\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Single input with/without multiple input\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Single Thread\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_thread \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m         augmented_results \u001b[38;5;241m=\u001b[39m [\u001b[43maction_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_data\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# Multi Thread\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m         augmented_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parallel_augment(action_fx, clean_data, n\u001b[38;5;241m=\u001b[39mn, num_thread\u001b[38;5;241m=\u001b[39mnum_thread)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nlpaug/augmenter/word/synonym.py:139\u001b[0m, in \u001b[0;36mSynonymAug.substitute\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word_pos \u001b[38;5;129;01min\u001b[39;00m word_poses:\n\u001b[0;32m--> 139\u001b[0m         candidates\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m[\u001b[49m\u001b[43maug_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mword_pos\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    141\u001b[0m candidates \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m candidates \u001b[38;5;28;01mif\u001b[39;00m c\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m!=\u001b[39m original_token\u001b[38;5;241m.\u001b[39mlower()]\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(candidates) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nlpaug/model/word_dict/wordnet.py:46\u001b[0m, in \u001b[0;36mWordNet.predict\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, word, pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     45\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m synonym \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynsets\u001b[49m(word, pos\u001b[38;5;241m=\u001b[39mpos, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang):\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m lemma \u001b[38;5;129;01min\u001b[39;00m synonym\u001b[38;5;241m.\u001b[39mlemmas(lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang):\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_synonym:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:116\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:78\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    651\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/root/nltk_data'\n**********************************************************************"],"ename":"LookupError","evalue":"\n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/root/nltk_data'\n**********************************************************************","output_type":"error"}]},{"cell_type":"code","source":"# Assuming dn is an instance of the data_normalization class\npd = preprocess_data()\n\n# Apply the normalize_text method to the review column and replace the values\ncleaned_data['Review'] = cleaned_data['Review'].apply(lambda x: pd.preprocess_text(x))\n\ncleaned_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T20:26:32.255189Z","iopub.execute_input":"2024-05-19T20:26:32.255697Z","iopub.status.idle":"2024-05-19T20:26:36.786875Z","shell.execute_reply.started":"2024-05-19T20:26:32.255667Z","shell.execute_reply":"2024-05-19T20:26:36.785687Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   No.                                             Review      aspek sentimen\n0    1                      makanan pelayanan mantap baik  pelayanan  positif\n1    1                      makanan pelayanan mantap baik    makanan  positif\n2    2  bohong mc donalds ayam spicy mc flurry kadang ...       rasa  positif\n3    2  bohong mc donalds ayam spicy mc flurry kadang ...       menu  positif\n4    3  ubah higines anak anak suka makan ayam goreng ...       rasa  positif","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>No.</th>\n      <th>Review</th>\n      <th>aspek</th>\n      <th>sentimen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>makanan pelayanan mantap baik</td>\n      <td>pelayanan</td>\n      <td>positif</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>makanan pelayanan mantap baik</td>\n      <td>makanan</td>\n      <td>positif</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>bohong mc donalds ayam spicy mc flurry kadang ...</td>\n      <td>rasa</td>\n      <td>positif</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>bohong mc donalds ayam spicy mc flurry kadang ...</td>\n      <td>menu</td>\n      <td>positif</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>ubah higines anak anak suka makan ayam goreng ...</td>\n      <td>rasa</td>\n      <td>positif</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\n\nclass Config:\n    def __init__(self):\n        self.batch_size = 32\n        self.val_size = 0.1\n\nconfig = Config()\n\n# Splitting the dataset\ntrain_data, val_data = train_test_split(cleaned_data, test_size=config.val_size, random_state=42)\n\n# Creating DataLoader instances\ntrain_loader = DataLoader(train_data.to_dict('records'), batch_size=config.batch_size, shuffle=True)\nval_loader = DataLoader(val_data.to_dict('records'), batch_size=config.batch_size, shuffle=False)\n\n# Print to verify the splits\nprint(f\"Training set size: {len(train_data)}\")\nprint(f\"Validation set size: {len(val_data)}\")\n\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T20:26:36.788022Z","iopub.execute_input":"2024-05-19T20:26:36.788354Z","iopub.status.idle":"2024-05-19T20:26:38.652675Z","shell.execute_reply.started":"2024-05-19T20:26:36.788314Z","shell.execute_reply":"2024-05-19T20:26:38.651699Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Training set size: 3336\nValidation set size: 371\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"       No.                                             Review      aspek  \\\n194    111  mantap penuh bmku pelayanan ekstra ramah proto...  pelayanan   \n1417   730  salah tempat nongkrong baik makan yakin kualit...     tempat   \n2459  1255  pariasi makanan sesuai nama restonya darmaga s...    makanan   \n2293  1173  makanan menu agam buharga jangkau tempat luas ...    makanan   \n3251  1691  pandang bagus makanan harga jangkau tempat rek...    makanan   \n\n     sentimen  \n194   positif  \n1417  positif  \n2459   netral  \n2293  positif  \n3251  positif  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>No.</th>\n      <th>Review</th>\n      <th>aspek</th>\n      <th>sentimen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>194</th>\n      <td>111</td>\n      <td>mantap penuh bmku pelayanan ekstra ramah proto...</td>\n      <td>pelayanan</td>\n      <td>positif</td>\n    </tr>\n    <tr>\n      <th>1417</th>\n      <td>730</td>\n      <td>salah tempat nongkrong baik makan yakin kualit...</td>\n      <td>tempat</td>\n      <td>positif</td>\n    </tr>\n    <tr>\n      <th>2459</th>\n      <td>1255</td>\n      <td>pariasi makanan sesuai nama restonya darmaga s...</td>\n      <td>makanan</td>\n      <td>netral</td>\n    </tr>\n    <tr>\n      <th>2293</th>\n      <td>1173</td>\n      <td>makanan menu agam buharga jangkau tempat luas ...</td>\n      <td>makanan</td>\n      <td>positif</td>\n    </tr>\n    <tr>\n      <th>3251</th>\n      <td>1691</td>\n      <td>pandang bagus makanan harga jangkau tempat rek...</td>\n      <td>makanan</td>\n      <td>positif</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# Assuming cleaned_data is your dataframe\n# Convert categorical aspect labels to numerical labels\naspect_label_encoder = LabelEncoder()\ncleaned_data['aspek'] = aspect_label_encoder.fit_transform(cleaned_data['aspek'])\n\n# Check the class mapping for aspects\nprint(\"Aspect Classes:\", aspect_label_encoder.classes_)\n\n# If you still need to convert 'sentimen' column to numerical labels as well\nsentiment_label_encoder = LabelEncoder()\ncleaned_data['sentimen'] = sentiment_label_encoder.fit_transform(cleaned_data['sentimen'])\n\n# Check the class mapping for sentiments\nprint(\"Sentiment Classes:\", sentiment_label_encoder.classes_)\n\ncleaned_data.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-19T20:26:38.654052Z","iopub.execute_input":"2024-05-19T20:26:38.654564Z","iopub.status.idle":"2024-05-19T20:26:38.670311Z","shell.execute_reply.started":"2024-05-19T20:26:38.654529Z","shell.execute_reply":"2024-05-19T20:26:38.669372Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Aspect Classes: ['harga' 'makanan' 'menu' 'pelayanan' 'rasa' 'tempat']\nSentiment Classes: ['negatif' 'netral' 'positif']\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"   No.                                             Review  aspek  sentimen\n0    1                      makanan pelayanan mantap baik      3         2\n1    1                      makanan pelayanan mantap baik      1         2\n2    2  bohong mc donalds ayam spicy mc flurry kadang ...      4         2\n3    2  bohong mc donalds ayam spicy mc flurry kadang ...      2         2\n4    3  ubah higines anak anak suka makan ayam goreng ...      4         2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>No.</th>\n      <th>Review</th>\n      <th>aspek</th>\n      <th>sentimen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>makanan pelayanan mantap baik</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>makanan pelayanan mantap baik</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>bohong mc donalds ayam spicy mc flurry kadang ...</td>\n      <td>4</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>bohong mc donalds ayam spicy mc flurry kadang ...</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>ubah higines anak anak suka makan ayam goreng ...</td>\n      <td>4</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom tqdm import tqdm\n\n# Tokenizer\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n\n# Prepare the dataset class\nclass ReviewDataset(Dataset):\n    def __init__(self, data, tokenizer, max_len):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        review = str(self.data.iloc[idx]['Review'])\n        aspect = self.data.iloc[idx]['aspek']\n\n        encoding = self.tokenizer.encode_plus(\n            review,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'aspect': torch.tensor(aspect, dtype=torch.long)\n        }\n\n# Load and preprocess the data\n# Assuming cleaned_data is already defined as a DataFrame\n\n# Encode aspects\naspect_label_encoder = LabelEncoder()\ncleaned_data['aspek'] = aspect_label_encoder.fit_transform(cleaned_data['aspek'])\n\n# Prepare Data\ntrain_data, val_data = train_test_split(cleaned_data, test_size=0.1, random_state=42)\ntrain_dataset = ReviewDataset(train_data, tokenizer, max_len=128)\nval_dataset = ReviewDataset(val_data, tokenizer, max_len=128)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Initialize the model\nconfig = BertConfig.from_pretrained('indobenchmark/indobert-base-p1', num_labels=len(aspect_label_encoder.classes_))\nmodel = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', config=config)\n\n# Training\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n\nfor epoch in range(5):  # 30 epochs\n    model.train()\n    total_loss = 0\n    predictions = []\n    true_labels = []\n\n    for batch in tqdm(train_loader):\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        aspect = batch['aspect'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=aspect)\n        loss = outputs.loss\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n        # Collect predictions and true labels for metrics\n        _, preds = torch.max(outputs.logits, dim=1)\n        predictions.extend(preds.cpu().numpy())\n        true_labels.extend(aspect.cpu().numpy())\n\n    # Calculate metrics for the current epoch\n    predicted_aspects = aspect_label_encoder.inverse_transform(predictions)\n    true_aspects = aspect_label_encoder.inverse_transform(true_labels)\n\n    accuracy = accuracy_score(true_aspects, predicted_aspects)\n    precision = precision_score(true_aspects, predicted_aspects, average='weighted')\n    recall = recall_score(true_aspects, predicted_aspects, average='weighted')\n    f1 = f1_score(true_aspects, predicted_aspects, average='weighted')\n\n    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n\n# Prediction on validation set\nmodel.eval()\npredictions = []\ntrue_labels = []\nwith torch.no_grad():\n    for batch in tqdm(val_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        true_aspects = batch['aspect'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        _, preds = torch.max(outputs.logits, dim=1)\n        predictions.extend(preds.cpu().numpy())\n        true_labels.extend(true_aspects.cpu().numpy())\n\n# Decode predictions to original aspect labels\npredicted_aspects = aspect_label_encoder.inverse_transform(predictions)\ntrue_aspects = aspect_label_encoder.inverse_transform(true_labels)\n\n# Calculate metrics\naccuracy = accuracy_score(true_aspects, predicted_aspects)\nprecision = precision_score(true_aspects, predicted_aspects, average='weighted')\nrecall = recall_score(true_aspects, predicted_aspects, average='weighted')\nf1 = f1_score(true_aspects, predicted_aspects, average='weighted')\n\nprint(f\"Validation Accuracy: {accuracy:.4f}\")\nprint(f\"Validation Precision: {precision:.4f}\")\nprint(f\"Validation Recall: {recall:.4f}\")\nprint(f\"Validation F1 Score: {f1:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-19T20:30:22.916209Z","iopub.execute_input":"2024-05-19T20:30:22.916553Z","iopub.status.idle":"2024-05-19T20:33:54.334772Z","shell.execute_reply.started":"2024-05-19T20:30:22.916525Z","shell.execute_reply":"2024-05-19T20:33:54.333846Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n100%|██████████| 105/105 [00:41<00:00,  2.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 1.2481, Accuracy: 0.4559, Precision: 0.4570, Recall: 0.4559, F1 Score: 0.4440\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 105/105 [00:41<00:00,  2.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Loss: 0.9313, Accuracy: 0.5168, Precision: 0.5187, Recall: 0.5168, F1 Score: 0.5153\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 105/105 [00:41<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Loss: 0.8899, Accuracy: 0.5210, Precision: 0.5214, Recall: 0.5210, F1 Score: 0.5193\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 105/105 [00:41<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Loss: 0.8796, Accuracy: 0.5192, Precision: 0.5200, Recall: 0.5192, F1 Score: 0.5163\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 105/105 [00:41<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Loss: 0.8580, Accuracy: 0.5207, Precision: 0.5228, Recall: 0.5207, F1 Score: 0.5191\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12/12 [00:01<00:00,  7.11it/s]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.4636\nValidation Precision: 0.4370\nValidation Recall: 0.4636\nValidation F1 Score: 0.4154\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntrain_losses.append(epoch_loss)\ntrain_accuracies.append(epoch_accuracy)\n\n\n# Plot the training loss and accuracy\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, 31), train_losses, label='Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss over Epochs')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, 31), train_accuracies, label='Training Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training Accuracy over Epochs')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T20:28:52.637142Z","iopub.execute_input":"2024-05-19T20:28:52.637668Z","iopub.status.idle":"2024-05-19T20:28:53.007416Z","shell.execute_reply.started":"2024-05-19T20:28:52.637640Z","shell.execute_reply":"2024-05-19T20:28:53.006318Z"},"trusted":true},"execution_count":11,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain_losses\u001b[49m\u001b[38;5;241m.\u001b[39mappend(epoch_loss)\n\u001b[1;32m      4\u001b[0m train_accuracies\u001b[38;5;241m.\u001b[39mappend(epoch_accuracy)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Plot the training loss and accuracy\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"],"ename":"NameError","evalue":"name 'train_losses' is not defined","output_type":"error"}]}]}